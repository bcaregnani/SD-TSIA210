{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iafPdtuncbq7"
   },
   "source": [
    "# TP: MNIST with Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlKZ3Hnas7B4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow version 2.11.0\n",
      "Using keras version 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"Using tensorflow version \" + str(tf.__version__))\n",
    "print(\"Using keras version \" + str(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_QLz9_jbRZq"
   },
   "source": [
    "## Loading and preparing the MNIST dataset\n",
    "Load the MNIST dataset made available by keras.datasets. Check the size of the training and testing sets. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "gG83hGyVmijn"
   },
   "outputs": [],
   "source": [
    "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
    "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
    "from keras.datasets import mnist\n",
    "(x1, y1), (x2, y2) = mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRPbU_Z4U6Ac"
   },
   "source": [
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "Using the pyplot package, visualize the first sample of the training set:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VAu7oW0Zu4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f41ec4b41c0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize the first training sample using the Matplotlib library with the imshow function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x1[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7YsRekMVDg-"
   },
   "source": [
    "The database contains images of handwritten digits. Hence, they belong to one of 10 categories, depending on the digit they represent. \n",
    "Reminder: in order to do multi-class classification, we use the softmax function, which outputs a multinomial probability distribution. That means that the output to our model will be a vector of size $10$, containing probabilities (meaning that the elements of the vector will be positive sum to $1$).\n",
    "For easy computation, we want to true labels to be represented with the same format: that is what we call **one-hot encoding**. For example, if an image $\\mathbf{x}$ represents the digit $5$, we have the corresponding one_hot label (careful, $0$ will be the first digit): \n",
    "$$ \\mathbf{y} = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] $$\n",
    "Here, you need to turn train and test labels to one-hot encoding using the following function: \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQbkllF8mnaf"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train=to_categorical(y1,10)\n",
    "y_test=to_categorical(y2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jv29YLtVO3q"
   },
   "source": [
    "Images are black and white, with size $28 \\times 28$. We will work with them using a simple linear classification model, meaning that we will have them as vectors of size $(784)$.\n",
    "You should then transform the images to the size $(784)$ using the numpy function ```reshape```.\n",
    "\n",
    "Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation. Be careful to your methodology: while you have access to training data, you may not have access to testing data, and must avoid using any statistic on the testing dataset.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptTRSDo5nJyZ"
   },
   "outputs": [],
   "source": [
    "# Reshape images to vectors of pixels\n",
    "img_rows, img_cols = x1.shape[1], x1.shape[2]\n",
    "x1 =  x1.reshape(x1.shape[0], img_cols*img_rows)\n",
    "x2 = x2.reshape(x2.shape[0],img_rows*img_cols)\n",
    "\n",
    "\n",
    "# Cast pixels from uint8 to float32\n",
    "x1 = x1.astype('float32')\n",
    "\n",
    "# Now let us normalize the images so that they have zero mean and standard deviation\n",
    "# Hint: are real testing data statistics known at training time ?\n",
    "\n",
    "mean=np.mean(x1)\n",
    "std=np.std(x1)\n",
    "\n",
    "x_train = (x1 - mean)/std\n",
    "\n",
    "x_test = (x2 - mean)/std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part: working with Numpy\n",
    "\n",
    "Look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf) for some basic information on how to use numpy.\n",
    "\n",
    "### Defining the model \n",
    "\n",
    "We will here create a simple, linear classification model. We will take each pixel in the image as an input feature (making the size of the input to be $784$) and transform these features with a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$. Since there is $10$ possible classes, we want to obtain $10$ scores. Then, \n",
    "$$ \\mathbf{W} \\in \\mathbb{R}^{784 \\times 10} $$\n",
    "$$ \\mathbf{b} \\in \\mathbb{R}^{10} $$\n",
    "\n",
    "and our scores are obtained with:\n",
    "$$ \\mathbf{z} = \\mathbf{W}^{T} \\mathbf{x} +  \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^{784}$ is the input vector representing an image.\n",
    "We note $\\mathbf{y} \\in \\mathbb{R}^{10}$ as the target one_hot vector. \n",
    "\n",
    "Here, you fist need to initialize $\\mathbf{W}$ and $\\mathbf{b}$ using ```np.random.normal``` and ```np.zeros```, then compute $\\mathbf{z}$.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid implementing a complicated gradient back-propagation,\n",
    "# we will try a very simple architecture with one layer \n",
    "def initLayer(n_input,n_output):\n",
    "    \"\"\"\n",
    "    Initialize the weights, return the number of parameters\n",
    "    Inputs: n_input: the number of input units - int\n",
    "          : n_output: the number of output units - int\n",
    "    Outputs: W: a matrix of weights for the layer - numpy ndarray\n",
    "           : b: a vector bias for the layer - numpy ndarray\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create W at the right size with a normal distribution\n",
    "    W = np.random.normal(size=(n_input,n_output))\n",
    "    # Create b at the right size, with zeros\n",
    "    b = np.zeros(n_output)\n",
    "    nb_params = n_input*n_output + n_output\n",
    "    return W, b, nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = x_train.shape[0] \n",
    "n_feature = x_train.shape[1]\n",
    "n_labels = 10\n",
    "W, b, nb_params = initLayer(n_feature, n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, b, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: W: the weights - numpy ndarray\n",
    "          : b: the bias - numpy ndarray\n",
    "          : X: the batch - numpy ndarray\n",
    "    Outputs: z: outputs - numpy ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    z = np.dot(W.T,X)+b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the output \n",
    "\n",
    "To obtain classification probabilities, we use the softmax function:\n",
    "$$ \\mathbf{o} = softmax(\\mathbf{z}) \\text{         with          } o_i = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)} $$\n",
    "\n",
    "The usual difficulty with the softmax function is the possibility of overflow when the scores $z_i$ are already large. Since a softmax is not affected by a shift affecting the whole vector $\\mathbf{z}$:\n",
    "$$ \\frac{\\exp(z_i - c)}{\\sum_{j=0}^{9} \\exp(z_j - c)} =  \\frac{\\exp(c) \\exp(z_i)}{\\exp(c) \\sum_{j=0}^{9} \\exp(z_j)} = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)}$$\n",
    "what trick can we use to ensure we will not encounter any overflow ? \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Perform the softmax transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - numpy ndarray\n",
    "    Outputs: out: the activation values - numpy ndarray\n",
    "    \"\"\"\n",
    "    z2=z-np.max(z)\n",
    "    out = np.exp(z2)/np.sum(np.exp(z2))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making updates\n",
    "\n",
    "We define a learning rate $\\eta$. The goal is to be able to apply updates:\n",
    "$$ \\mathbf{W}^{t+1} = \\mathbf{W}^{t} + \\nabla_{\\mathbf{W}} l_{MLE} $$\n",
    "\n",
    "In order to do this, we will compute this gradient (and the bias) in the function ```update```. In the next function ```updateParams```, we will actually apply the update with regularization. \n",
    "\n",
    "Reminder: the gradient $\\nabla_{\\mathbf{W}} l_{MLE}$ is the matrix containing the partial derivatives \n",
    "$$ \\left[\\frac{\\delta l_{MLE}}{\\delta W_{ij}}\\right]_{i=1..784, j=1..10} $$\n",
    "**Remark**: Careful, the usual way of implementing this in python has the dimensions of $\\mathbf{W}$ reversed compared to the notation of the slides.\n",
    "\n",
    "Coordinate by coordinate, we obtain the following update: \n",
    "$$ W_{ij}^{t+1} = W_{ij}^{t} + \\eta \\frac{\\delta l_{MLE}}{\\delta W_{ij}} $$\n",
    "\n",
    "Via the chain rule, we obtain, for an input feature $i \\in [0, 783]$ and a output class $j \\in [0, 9]$: $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = \\frac{\\delta l_{MLE}}{\\delta z_{j}} \\frac{\\delta z_j}{\\delta W_{ij}}$$ \n",
    "\n",
    "It's easy to compute that $\\frac{\\delta z_j}{\\delta W_{ij}} = x_i$\n",
    "\n",
    "We compute the softmax derivative, to obtain:\n",
    "$$ \\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y} $$\n",
    "\n",
    "Hence, $\\frac{\\delta l_{MLE}}{\\delta z_{j}} = o_j - y_j$ and we obtain that $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = (o_j - y_j) x_i$$\n",
    "\n",
    "This can easily be written as a scalar product, and a similar computation (even easier, actually) can be done for $\\mathbf{b}$. Noting $\\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y}$ as ```grad``` in the following function, compute the gradients $\\nabla_{\\mathbf{W}} l_{MLE}$ and $\\nabla_{\\mathbf{b}} l_{MLE}$ in order to call the function ```updateParams```.\n",
    "\n",
    "Note: the regularizer and the weight_decay $\\lambda$ are used in ```updateParams```.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, W, b, grad, X, regularizer, weight_decay):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : W: the weights - ndarray\n",
    "          : b: the bias -  ndarray\n",
    "          : grad: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : X: the data -  ndarray\n",
    "          : regularizer: 'L2' or None - the regularizer to be used in updateParams\n",
    "          : weight_decay: the weight decay to be used in updateParams - float\n",
    "    Outputs: W: the weights updated -  ndarray\n",
    "           : b: the bias updated -  ndarray\n",
    "    \"\"\"\n",
    "    grad_w = np.dot(X.reshape(-1,1),grad.reshape(1,-1))\n",
    "    grad_b = grad\n",
    "        \n",
    "    W = updateParams(W, grad_w, eta, regularizer, weight_decay)\n",
    "    b = updateParams(b, grad_b, eta, regularizer, weight_decay)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule is affected by regularization. We implement two cases: No regularization, or L2 regularization. Use the two possible update rules to implement the following function: <div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(param, grad_param, eta, regularizer=None, weight_decay=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: param: the network parameters - ndarray\n",
    "          : grad_param: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : weight_decay: the weight-decay - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    if regularizer==None:\n",
    "        grad = param - eta*grad_param\n",
    "        return grad\n",
    "    elif regularizer=='L2':\n",
    "        grad = param - eta*(grad_param + weight_decay*param)\n",
    "        return grad\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Accuracy\n",
    "\n",
    "Here, we simply use the model to predict the class (by taking the argmax of the output !) for every example in ```X```, and count the number of times the model is right, to output the accuracy.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAcc(W, b, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    \n",
    "    accuracy=0\n",
    "\n",
    "    for j in range(X.shape[0]):\n",
    "        ### Forward propagation\n",
    "        z = forward(W,b,X[j])\n",
    " \n",
    "        ### Compute the softmax and the prediction\n",
    "        out = softmax(z)\n",
    "        pred = to_categorical(np.argmax(out),10)\n",
    "    \n",
    "        ### Compute the accuracy\n",
    "        if np.all(pred == labels[j]):\n",
    "            accuracy += 1\n",
    "      \n",
    "    return accuracy/len(labels)\n",
    "      \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training\n",
    "\n",
    "The following hyperparameters are given. Next, we can assemble all the function previously defined to implement a training loop. We will train the classifier on **one epoch**, meaning that the model will see each training example once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "eta = 0.01\n",
    "regularizer = 'L2'\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Training\n",
    "log_interval = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.10168333333333333 0.1014 0.01\n",
      "5000 0.8101833333333334 0.8225 0.01\n",
      "10000 0.8354833333333334 0.8406 0.01\n",
      "15000 0.8522166666666666 0.8594 0.01\n",
      "20000 0.8493166666666667 0.854 0.01\n",
      "25000 0.85505 0.8638 0.01\n"
     ]
    }
   ],
   "source": [
    "# Data structures for plotting\n",
    "g_train_acc=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#######################\n",
    "### Learning process ##\n",
    "#######################\n",
    "for j in range(n_training):\n",
    "    # Getting the example\n",
    "    X, y = x_train[j],y_train[j]\n",
    "\n",
    "    # Forward propagation\n",
    "    z = forward(W, b, X)\n",
    "\n",
    "    # Compute the softmax\n",
    "    out =softmax(z)\n",
    "        \n",
    "    # Compute the gradient at the top layer\n",
    "    derror = out - y # This is o - y \n",
    "\n",
    "    # Update the parameters\n",
    "    W, b = update(eta, W, b, derror, X, regularizer, weight_decay)\n",
    "\n",
    "    if j % log_interval == 0:\n",
    "        # Every log_interval examples, look at the training accuracy\n",
    "        train_accuracy = computeAcc(W, b, x_train, y_train) \n",
    "\n",
    "        # And the testing accuracy\n",
    "        test_accuracy = computeAcc(W, b, x_test, y_test) \n",
    "\n",
    "        g_train_acc.append(train_accuracy)\n",
    "        g_valid_acc.append(test_accuracy)\n",
    "        result_line = str(int(j)) + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "        print(result_line)\n",
    "\n",
    "g_train_acc.append(train_accuracy)\n",
    "g_valid_acc.append(test_accuracy)\n",
    "result_line = \"Final result:\" + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "print(result_line)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the performance of this simple linear classifier ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part: Autoencoder with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder and PCA\n",
    "\n",
    "First, we will try to connect the representation produced by Principal Component Analysis with what is learnt by a simple, linear, autoencoder. We will use the ```scikit-learn``` implementation of the ```PCA``` to obtain the two first components (hint: use the attribute ```.components_```), and visualize them:\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Second Principal Component')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's find the first 2 PCA components\n",
    "num_components = 2\n",
    "pca = PCA(n_components=num_components).fit(x_train)\n",
    "\n",
    "# Reshape so they resemble images and we can print them\n",
    "eigen_mnist = pca.components_.reshape(num_components, 28, 28)\n",
    "\n",
    "# Show the reshaped principal components\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(eigen_mnist[0], cmap='gray')\n",
    "ax[0].set_xlabel('First Principal Component')\n",
    "ax[1].imshow(eigen_mnist[1], cmap='gray')\n",
    "ax[1].set_xlabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.901363, 39.411995], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the variance explained by those components\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the visualization in relation to the variance explained by only keeping the two principal components:\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Autoencoder with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use Keras to implement the autoencoder. You can take a look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf) for some basic commands to use keras.\n",
    "\n",
    "In this first case, we implement a **simple linear autoencoder**. Build it in order to have the same capacity as the PCA decomposition (2 hidden dimensions !) we made just above. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 784)]             0         \n",
      "                                                                 \n",
      " latent_view (Dense)         (None, 2)                 1570      \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 784)               2352      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,922\n",
      "Trainable params: 3,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 16:44:17.991129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-07 16:44:17.991162: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-07 16:44:17.991191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bruno-HP): /proc/driver/nvidia/version does not exist\n",
      "2023-04-07 16:44:17.994240: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Input layer\n",
    "input_layer = Input(shape=(784,), name='input_layer')\n",
    "\n",
    "# Encoding layer\n",
    "latent_view = Dense(2, activation='linear', name='latent_view')(input_layer)\n",
    "\n",
    "# Decoding layer\n",
    "output_layer = Dense(784, activation='sigmoid', name='output_layer')(latent_view)\n",
    "\n",
    "ae_model = Model(input_layer, output_layer, name='ae_model')\n",
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What loss shoud we use ? Choose the usual one and import it directly from Keras. You can use a simple ```SGD``` optimizer, and then compile the model; finally, train it to rebuild images from the original examples. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "2023-04-07 16:54:29.423789: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 16:54:29.551475: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 3s 5ms/step - loss: 1.2487 - val_loss: 1.2565\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 1.2433 - val_loss: 1.2469\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.2236 - val_loss: 1.2109\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 1.1601 - val_loss: 1.1115\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.0334 - val_loss: 0.9659\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.9106 - val_loss: 0.8749\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.8520 - val_loss: 0.8402\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.8293 - val_loss: 0.8259\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.8190 - val_loss: 0.8186\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.8134 - val_loss: 0.8144\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "loss = MeanSquaredError()\n",
    "\n",
    "optimizer = SGD(lr=1e-1) \n",
    "ae_model.compile(optimizer=optimizer, loss=loss) \n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "# No noise here - we want to train a simple auto-encoder and compare visually with PCA\n",
    "history = ae_model.fit(x_train,\n",
    "                       x_train,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=1,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the name of your layer (obtained through the command ```model.summary()```) is ```'layer'```, here is the way to obtained the weights. Visualize the weights of the encoder and compare them to the two components obtained through the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: layer. Existing layers are: ['input_layer', 'latent_view', 'output_layer'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bruno/Telecom/TSIA/SD-TSIA205/TP/TP3/tpp.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/bruno/Telecom/TSIA/SD-TSIA205/TP/TP3/tpp.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m weights, bias \u001b[39m=\u001b[39m ae_model\u001b[39m.\u001b[39;49mget_layer(\u001b[39m'\u001b[39;49m\u001b[39mlayer\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mget_weights()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:3353\u001b[0m, in \u001b[0;36mModel.get_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   3351\u001b[0m         \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m name:\n\u001b[1;32m   3352\u001b[0m             \u001b[39mreturn\u001b[39;00m layer\n\u001b[0;32m-> 3353\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3354\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo such layer: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m. Existing layers are: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3355\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(layer\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3356\u001b[0m     )\n\u001b[1;32m   3357\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3358\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mProvide either a layer name or layer index at `get_layer`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3359\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: layer. Existing layers are: ['input_layer', 'latent_view', 'output_layer']."
     ]
    }
   ],
   "source": [
    "weights, bias = ae_model.get_layer('layer').get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the two dimensions of the encoder, in a similar manner to the principal components\n",
    "# (after reshaping them as images !)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualize the images rebuilt by the network !\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few images at random: look from n\n",
    "n = np.random.randint(0,len(test_images)-5)\n",
    "\n",
    "# Plot a few images from n  \n",
    "f, ax = plt.subplots(1,5)\n",
    "for i,a in enumerate(range(n,n+5)):\n",
    "    ...\n",
    "    \n",
    "# Get the prediction from the model \n",
    "\n",
    "\n",
    "# ... and plot them \n",
    "f, ax = plt.subplots(1,5)\n",
    "for i,a in enumerate(range(n,n+5)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same ( = build a new model) with a latent dimension that is largely higher than 2. Compare the visualizations and the images that are rebuilt. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: De-noising Autoencoder\n",
    "\n",
    "Now, we can implement a **de-noising autoencoder**. The following function will transform an array of images by adding it random noise. Create a new autoencoder model, this time with **more layers** and **non-linear activations** (like the ReLU) and train it to rebuild the de-noised images. Display some testing images, with noise, and re-built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(array):\n",
    "    \"\"\"\n",
    "    Adds random noise to each image in the supplied array.\n",
    "    \"\"\"\n",
    "    noise_factor = 0.4\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "    return noisy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data with added noise\n",
    "noisy_train_images = noise(train_images)\n",
    "noisy_test_images = noise(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the images with noise against the originals\n",
    "\n",
    "\n",
    "# Build a new model with more layers and Relu activations\n",
    "\n",
    "\n",
    "# Compile it but here, use noised data as inputs !\n",
    "\n",
    "\n",
    "# Visualize the images rebuilt by the model !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we normalize the images to be in the 0-1 range, what other loss function could we use ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP4_1_empty.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
